{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <span style=\"color:teal\"> Introduction to surrogate modelling in the geosciences </span>\n",
    "\n",
    "#### Marc Bocquet¹ [marc.bocquet@enpc.fr](mailto:marc.bocquet@enpc.fr) and Alban Farchi¹ [alban.farchi@enpc.fr](mailto:alban.farchi@enpc.fr)\n",
    "##### (1) CEREA, École des Ponts and EdF R&D, IPSL, Île-de-France, France\n",
    "\n",
    "During this session, we will apply standard machine learning methods to learn the dynamics of the Lorenz 1996 model. The objective here is to get a preview of how machine learning can be applied to geoscientific models in a low-order models where testing is quick."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> The Lorenz 1996 model </span>\n",
    "\n",
    "The Lorenz 1996 (L96, [Lorenz and Emanuel 1998](https://journals.ametsoc.org/view/journals/atsc/55/3/1520-0469_1998_055_0399_osfswo_2.0.co_2.xml)) is a low-order chaotic model commonly used in data assimilation to asses the performance of new algorithms. It represents the evolution of some unspecified scalar meteorological quantity (perhaps vorticity or temperature) over a latitude circle.\n",
    "\n",
    "The model **dynamics** is driven by the following set of ordinary differential equations (ODEs):\n",
    "$$\n",
    "    \\forall n \\in [1, N_{x}], \\quad \\frac{\\mathrm{d}x_{n}}{\\mathrm{d}t} =\n",
    "    (x_{n+1}-x_{n-2})x_{n-1}-x_{n}+F,\n",
    "$$\n",
    "where the indices are periodic: $x_{-1}=x_{N_{x}-1}$, $x_{0}=x_{N_{x}}$, and $x_{1}=x_{N_{x}+1}$, and where the system size $N_{x}$ can take arbitrary values.\n",
    "\n",
    "In the standard configuration, $N_{x}=40$ and the forcing coefficient is $F=8$. The ODEs are integrated using a fourth-order Runge-Kutta scheme with a time step of $0.05$ model time unit (MTU). The resulting dynamics is **chaotic** with a doubling time of errors around $0.42$ MTU (the corresponding Lyapunov is hence $0.61$ MTU). For comparison, $0.05$ MTU represent six hours of real time and correspond to an average autocorrelation around $0.967$. Finally, the model variability (spatial average of the time standard deviation per variable) is $3.64$.\n",
    "\n",
    "In this series of experiments, we will try to emulate the dynamics of the L96 model using artificial neural networks (NN).\n",
    "1. We start by running the **true model** to build a training dataset.\n",
    "2. We build and **train neural networks** using this dataset.\n",
    "3. We evaluate the **forecast skill** of the surrogate models (the NNs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:green\"> The true model dynamics </span>\n",
    "\n",
    "Before building the training dataset, let us illustrate the model dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> Importing all modules and define some visualisation functions</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "def plot_l96_traj(\n",
    "    x,\n",
    "    model,\n",
    "    linewidth,\n",
    "):\n",
    "    fig = plt.figure(figsize=(linewidth, linewidth/3))\n",
    "    plt.grid(False)\n",
    "    im = plt.imshow(\n",
    "        x.T, \n",
    "        aspect = 'auto',\n",
    "        origin = 'lower',\n",
    "        interpolation = 'spline36',\n",
    "        cmap = sns.diverging_palette(240, 60, as_cmap=True),\n",
    "        extent = [0, model.dt*x.shape[0], 0, model.Nx],\n",
    "        vmin = -10,\n",
    "        vmax = 15,\n",
    "    )\n",
    "    plt.colorbar(im)\n",
    "    plt.xlabel('Time (MTU)')\n",
    "    plt.ylabel('Lorenz 96 variables')\n",
    "    plt.tick_params(direction='out', left=True, bottom=True)\n",
    "    plt.show()\n",
    "\n",
    "def plot_l96_compare_traj(\n",
    "    x_ref,\n",
    "    x_pred,\n",
    "    model,\n",
    "    linewidth,\n",
    "):\n",
    "    error = x_pred - x_ref\n",
    "    fig = plt.figure(figsize=(linewidth, linewidth))\n",
    "    ax = plt.subplot(311)\n",
    "    ax.grid(False)\n",
    "    im = plt.imshow(\n",
    "        x_ref.T, \n",
    "        aspect = 'auto',\n",
    "        origin = 'lower',\n",
    "        interpolation = 'spline36',\n",
    "        cmap = sns.diverging_palette(240, 60, as_cmap=True),\n",
    "        extent = [0, model.dt*x_pred.shape[0], 0, model.Nx],\n",
    "        vmin = -10,\n",
    "        vmax = 15,\n",
    "    )\n",
    "    ax.set_title('true model integration')\n",
    "    plt.colorbar(im)\n",
    "    ax.set_ylabel('Lorenz 96 variables')\n",
    "    ax.tick_params(direction='out', left=True, bottom=True)\n",
    "    ax.set_xticklabels([])\n",
    "    ax = plt.subplot(312)\n",
    "    ax.grid(False)\n",
    "    im = plt.imshow(\n",
    "        x_pred.T,\n",
    "        aspect = 'auto',\n",
    "        origin = 'lower',\n",
    "        interpolation = 'spline36',\n",
    "        cmap = sns.diverging_palette(240, 60, as_cmap=True),\n",
    "        extent = [0, model.dt*x_pred.shape[0], 0, model.Nx],\n",
    "        vmin = -10,\n",
    "        vmax = 15,\n",
    "    )\n",
    "    ax.set_title('surrogate model integration')\n",
    "    plt.colorbar(im)\n",
    "    ax.set_ylabel('Lorenz 96 variables')\n",
    "    ax.tick_params(direction='out', left=True, bottom=True)\n",
    "    ax.set_xticklabels([])\n",
    "    ax = plt.subplot(313)\n",
    "    ax.grid(False)\n",
    "    im = ax.imshow(\n",
    "        error.T, \n",
    "        aspect = 'auto',\n",
    "        origin = 'lower',\n",
    "        interpolation = 'spline36',\n",
    "        cmap = sns.diverging_palette(240, 10, as_cmap=True),\n",
    "        extent = [0, model.dt*error.shape[0], 0, model.Nx],\n",
    "        vmin = -15,\n",
    "        vmax = 15,\n",
    "    )\n",
    "    ax.set_title('signed error')\n",
    "    plt.colorbar(im)\n",
    "    ax.set_xlabel('Time (MTU)')\n",
    "    ax.set_ylabel('Lorenz 96 variables')\n",
    "    ax.tick_params(direction='out', left=True, bottom=True)\n",
    "    plt.show()\n",
    "\n",
    "def get_plotly_color_palette(alpha=None):\n",
    "    if alpha is None:\n",
    "        return [\n",
    "            'rgb(99, 110, 250)',\n",
    "            'rgb(239, 85, 59)',\n",
    "            'rgb(0, 204, 150)',\n",
    "            'rgb(171, 99, 250)',\n",
    "            'rgb(255, 161, 90)',\n",
    "            'rgb(25, 211, 243)',\n",
    "            'rgb(255, 102, 146)',\n",
    "            'rgb(182, 232, 128)',\n",
    "            'rgb(255, 151, 255)',\n",
    "            'rgb(254, 203, 82)'\n",
    "        ]\n",
    "    else:\n",
    "        return [\n",
    "            f'rgba(99, 110, 250, {alpha})',\n",
    "            f'rgba(239, 85, 59, {alpha})',\n",
    "            f'rgba(0, 204, 150, {alpha})',\n",
    "            f'rgba(171, 99, 250, {alpha})',\n",
    "            f'rgba(255, 161, 90, {alpha})',\n",
    "            f'rgba(25, 211, 243, {alpha})',\n",
    "            f'rgba(255, 102, 146, {alpha})',\n",
    "            f'rgba(182, 232, 128, {alpha})',\n",
    "            f'rgba(255, 151, 255, {alpha})',\n",
    "            f'rgba(254, 203, 82, {alpha})'\n",
    "        ]\n",
    "\n",
    "def plot_l96_forecast_skill(\n",
    "    fss,\n",
    "    model,\n",
    "    p1,\n",
    "    p2,\n",
    "    xmax,\n",
    "    linewidth,\n",
    "):\n",
    "    fig = go.Figure()\n",
    "    palette = get_plotly_color_palette()\n",
    "    spalette = get_plotly_color_palette(alpha=0.2)\n",
    "    \n",
    "    for (index, key) in enumerate(fss):\n",
    "        \n",
    "        time = (model.dt/model.lyap_time)*np.arange(fss[key].shape[0])\n",
    "        rmse_m = fss[key].mean(axis=1) / model.model_var\n",
    "        rmse_p1 = np.percentile(fss[key], p1, axis=1) / model.model_var\n",
    "        rmse_p2 = np.percentile(fss[key], p2, axis=1) / model.model_var\n",
    "        \n",
    "        fig.add_scatter(\n",
    "            x=time,\n",
    "            y=rmse_m,\n",
    "            name=key,\n",
    "            customdata=np.arange(len(time)),\n",
    "            hovertemplate='index = %{customdata}, value = %{y:.3f}',\n",
    "            line_color=palette[index]\n",
    "        )\n",
    "        fig.add_scatter(\n",
    "            x=np.concatenate([time, time[::-1]]),\n",
    "            y=np.concatenate([rmse_p1, rmse_p2[::-1]]),\n",
    "            fill='toself',\n",
    "            name=key+' (CI)',\n",
    "            hoverinfo='skip',\n",
    "            fillcolor=spalette[index],\n",
    "            line_width=0,\n",
    "            mode='lines'\n",
    "        )        \n",
    "        \n",
    "    fig.update_xaxes(title_text='Time (Lyapunov time)')\n",
    "    fig.update_yaxes(title_text='normalised RMSE')\n",
    "    fig.update_layout(\n",
    "        title='Forecast skill', \n",
    "        xaxis_range=[0, xmax], \n",
    "        yaxis_range=[0, 2], \n",
    "        width=linewidth, \n",
    "        height=0.7*linewidth,\n",
    "        hovermode='x unified',\n",
    "    )\n",
    "    fig.add_hline(\n",
    "        y=np.sqrt(2), \n",
    "        line_width=1,\n",
    "        line_dash='dash',\n",
    "        line_color='black',\n",
    "        label_text=r'$\\sqrt{2}$',\n",
    "        label_textposition='start',\n",
    "    )\n",
    "    fig.show()\n",
    "    \n",
    "def plot_learning_curve(\n",
    "    loss,\n",
    "    val_loss,\n",
    "    title,\n",
    "    linewidth,\n",
    "):\n",
    "    \n",
    "    fig = go.Figure()\n",
    "    palette = get_plotly_color_palette()\n",
    "    \n",
    "    fig.add_scatter(\n",
    "        x=np.arange(len(loss)),\n",
    "        y=loss,\n",
    "        name='training loss',\n",
    "        customdata=np.arange(len(loss)),\n",
    "        hovertemplate='epoch = %{customdata}, value = %{y:.3f}',\n",
    "        line_color=palette[0]\n",
    "    )\n",
    "    \n",
    "    fig.add_scatter(\n",
    "        x=np.arange(len(val_loss)),\n",
    "        y=val_loss,\n",
    "        name='validation loss',\n",
    "        customdata=np.arange(len(val_loss)),\n",
    "        hovertemplate='epoch = %{customdata}, value = %{y:.3f}',\n",
    "        line_color=palette[1]\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text='Number of epochs')\n",
    "    fig.update_yaxes(title_text='MSE', type='log')\n",
    "    fig.update_layout(title=title, width=linewidth, height=0.7*linewidth, hovermode='x unified')\n",
    "\n",
    "    fig.show()\n",
    "\n",
    "class TQDMCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, desc, loss=None, val_loss=None):\n",
    "        super().__init__()\n",
    "        self.desc = desc\n",
    "        self.metrics = {'loss':loss, 'val_loss':val_loss}\n",
    "    \n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_bar = tqdm(total=self.params['epochs'], desc=self.desc)\n",
    "    \n",
    "    def on_train_end(self, logs=None):\n",
    "        self.epoch_bar.close()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        for name in self.metrics:\n",
    "            self.metrics[name]  = logs.get(name, self.metrics[name])\n",
    "        self.epoch_bar.set_postfix(mse=self.metrics['loss'], val_mse=self.metrics['val_loss'], refresh=False)\n",
    "        self.epoch_bar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Defining the true model </span>\n",
    "\n",
    "In the following cell, we define the true Lorenz 1996 model using standard values: \n",
    "- the number of variables $N_{x}$ is set to `Nx=40`;\n",
    "- the forcing coefficient $F$ is set to `F=8`;\n",
    "- the integration time step is set to `dt=0.05`.\n",
    "\n",
    "<span style=\"color:red\"> Exercise </span>\n",
    "- Implement the `tendency()` method of the `Lorenz1996Model` class.\n",
    "  This method should compute the model tendencies. You may use the\n",
    "  [`roll()`](https://numpy.org/doc/stable/reference/generated/numpy.roll.html)\n",
    "  function of `numpy`.\n",
    "- Implement the `forward()` method of the `Lorenz1996Model` class.\n",
    "  This method should compute an integration step forward in time.\n",
    "  The Runge--Kutta scheme is explained in the method's docstring.\n",
    "  A simple straightforward implementation with six statements is more \n",
    "  than enough for the present set of experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lorenz1996Model:\n",
    "    \"\"\"Implementation of the Lorenz 1996 model.\n",
    "    \n",
    "    Use the `tendency()` method to compute the model tendencies (i.e., dx/dt)\n",
    "    and use the `forward()` method to apply an integration step forward in time,\n",
    "    using a fourth order Runge--Kutta scheme.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    Nx : int\n",
    "        The number of variables in the model.\n",
    "    F : float\n",
    "        The model forcing.\n",
    "    dt : float\n",
    "        The model integration time step.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, Nx, F, dt):\n",
    "        \"\"\"Initialise the model.\"\"\"\n",
    "        self.Nx = Nx\n",
    "        self.F = F\n",
    "        self.dt = dt\n",
    "\n",
    "    def tendency(self, x):\n",
    "        \"\"\"Compute the model tendencies dx/dt.\n",
    "        \n",
    "        The tendencies are computed by batch using\n",
    "        `numpy` vectorisation.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray, shape (..., Nx)\n",
    "            Batch of input states.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        dx_dt : np.ndarray, shape (..., Nx)\n",
    "            Model tendencies computed at the input states.\n",
    "        \"\"\"\n",
    "        # TODO: implement it!\n",
    "        xp = np.roll(x, shift=-1, axis=-1)\n",
    "        xmm = np.roll(x, shift=+2, axis=-1)\n",
    "        xm = np.roll(x, shift=+1, axis=-1)\n",
    "        return (xp - xmm)*xm - x + self.F\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Apply an integration step forward in time.\n",
    "        \n",
    "        This method uses a fourth-order Runge--Kutta scheme:\n",
    "        k1 <- dx/dt at x\n",
    "        k2 <- dx/dt at x + dt/2*k1\n",
    "        k3 <- dx/dt at x + dt/2*k2\n",
    "        k4 <- dx/dt at x + dt*k3\n",
    "        k <- (k1 + 2*k2 + 2*k3 + k4)/6\n",
    "        x <- x + dt*k\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        x : np.ndarray, shape (..., Nx)\n",
    "            Batch of input states.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        integrated_x : np.ndarray, shape (..., Nx)\n",
    "            The integrated states after one step.\n",
    "        \"\"\"\n",
    "        # TODO: implement it!\n",
    "        k1 = self.tendency(x)\n",
    "        k2 = self.tendency(x+self.dt/2*k1)\n",
    "        k3 = self.tendency(x+self.dt/2*k2)\n",
    "        k4 = self.tendency(x+self.dt*k3)\n",
    "        k = (k1 + 2*k2 + 2*k3 + k4)/6\n",
    "        return x + self.dt*k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "true_model = Lorenz1996Model(Nx=40, dt=0.05, F=8)\n",
    "\n",
    "# save some statistics about the model\n",
    "true_model.model_var = 3.64\n",
    "true_model.doubling_time = 0.42\n",
    "true_model.lyap_time = 0.61"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Short model integration </span>\n",
    "\n",
    "In the following cells, we perform a rather short model integration, in order to illustrate the model dynamics. The initial condition is a random field.\n",
    "\n",
    "<span style=\"color:red\"> Exercise </span>\n",
    "- Implement the true model integration in the `perform_true_model_integration()` function.\n",
    "  A simple implementation with a `for-loop` should do the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_true_model_integration(Nt, Ne=1, seed=None):\n",
    "    \"\"\"Perform an integration in time using the true model.\n",
    "    \n",
    "    The initial state is a batch of random fields.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    Nt : int\n",
    "        The number of integration steps to perform.\n",
    "    Ne : int\n",
    "        The batch size.\n",
    "    seed : int\n",
    "        The random seed for the initialisation.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    xr : np.ndarray, shape (Nt+1, Ne, Nx)\n",
    "        The integrated batch of trajectories.\n",
    "    \"\"\"\n",
    "    # define rng\n",
    "    rng = np.random.default_rng(seed=seed)\n",
    "\n",
    "    # allocate memory\n",
    "    xt = np.zeros((Nt+1, Ne, true_model.Nx))\n",
    "\n",
    "    # initialisation\n",
    "    xt[0] = rng.normal(loc=3, scale=1, size=(Ne, true_model.Nx))\n",
    "    \n",
    "    # TODO: implement the model integration for Nt steps\n",
    "    for t in trange(Nt, desc='model integration'):\n",
    "        xt[t+1] = true_model.forward(xt[t])\n",
    "    \n",
    "    # return the trajectory\n",
    "    return xt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short model integration for visualisation purpose\n",
    "xt_plot = perform_true_model_integration(Nt=500, Ne=1, seed=3)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the trajectory\n",
    "plot_l96_traj(\n",
    "    xt_plot, \n",
    "    true_model,\n",
    "    linewidth=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see first a spin-up period of about $1$ MTU, where the initial condition is progressively forgotten and the trajectory progressively gets back to the model attractor. After this spin-up period, the dynamics is characterised by waves moving slowly towards the east (i.e. decreasing variable index). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> Prepare the dataset </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> A long model integration for the training data</span>\n",
    "\n",
    "We now use a true model trajectory to make the **training dataset**. This trajectory starts from a random field (different than the one used for the plotting trajectory) and we discard the first $100$ time steps to get rid of the spin-up process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# long model integration for the training data\n",
    "xt_train = perform_true_model_integration(Nt=10_000+100, Ne=1, seed=31)[:, 0]\n",
    "\n",
    "# discard the spin-up process\n",
    "xt_train = xt_train[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Preprocess the training data </span>\n",
    "\n",
    "The training dataset is made of input/output pairs, where the input is the state at a given time, and the output is the state at the following time.\n",
    "\n",
    "<span style=\"color:red\"> Exercise </span>\n",
    "- Implement the `extract_input_output()` function, in which the \n",
    "  neural network input and output are extracted from a given\n",
    "  trajectory. Use `numpy` slicing for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_input_output(xt):\n",
    "    # TODO: extract x (input)\n",
    "    x = xt[:-1]\n",
    "    # TODO: extract y (output)\n",
    "    y = xt[1:]\n",
    "    # return input/output\n",
    "    return (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract input/output from the training data\n",
    "x_train, y_train = extract_input_output(xt_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the normalisation using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute input/output mean/std\n",
    "x_mean = x_train.mean()\n",
    "y_mean = y_train.mean()\n",
    "x_std = x_train.std()\n",
    "y_std = y_train.std()\n",
    "\n",
    "# define normalisation/denormalisation functions\n",
    "def normalise_x(x):\n",
    "    return (x - x_mean)/x_std\n",
    "def normalise_y(y):\n",
    "    return (y - y_mean)/y_std\n",
    "def denormalise_x(x_norm):\n",
    "    return x_norm*x_std + x_mean\n",
    "def denormalise_y(y_norm):\n",
    "    return y_norm*y_std + y_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the training data is normalised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise the training data\n",
    "x_train_norm = normalise_x(x_train)\n",
    "y_train_norm = normalise_y(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Shorter model integrations for the validation and testing data</span>\n",
    "\n",
    "We repeat the same process to make the **validation** and **testing** data. In this case, the trajectory starts from two other random fields (and we still get rid of the spin-up processes) and can be somewhat shorter, but the normalisation must be the same as for the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short model integration for the validation data\n",
    "xt_valid = perform_true_model_integration(Nt=1_000+100, Ne=1, seed=314)[:, 0]\n",
    "\n",
    "# discard the spin-up process\n",
    "xt_valid = xt_valid[100:]\n",
    "\n",
    "# extract input/output from the validation data\n",
    "x_valid, y_valid = extract_input_output(xt_valid)\n",
    "\n",
    "# normalise the validation data\n",
    "x_valid_norm = normalise_x(x_valid)\n",
    "y_valid_norm = normalise_y(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# short model integration for the testing data\n",
    "xt_test = perform_true_model_integration(Nt=1_000+100, Ne=1, seed=3141)[:, 0]\n",
    "\n",
    "# discard the spin-up process\n",
    "xt_test = xt_test[100:]\n",
    "\n",
    "# extract input/output from the testing data\n",
    "x_test, y_test = extract_input_output(xt_test)\n",
    "\n",
    "# normalise the testing data\n",
    "x_test_norm = normalise_x(x_test)\n",
    "y_test_norm = normalise_y(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> An ensemble model integration for the forecast skill data</span>\n",
    "\n",
    "In order to assess the forecast skill of the surrogate model, we will use a different test dataset, in which we record an ensemble of **trajectories** (instead of an ensemble of input/output pairs). This will allow us to measure the accuracy of the forecast for longer integration times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensemble integration for the forecast skill data\n",
    "xt_fs = perform_true_model_integration(Nt=400+100, Ne=512, seed=31415)\n",
    "\n",
    "# discard the spin-up process\n",
    "xt_fs = xt_fs[100:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:green\"> The baseline model: persistence </span>\n",
    "\n",
    "In this first test series, we use **persistence** as surrogate model. This will provide baselines for our NN results. Persistence is defined as the model for which there is no time evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Evaluate the model</span>\n",
    "\n",
    "The mean square error (MSE) is the loss function that we will use to train our NNs later. Therefore, the test MSE is a measure of the efficiency of the learning/training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute test MSE\n",
    "test_mse_baseline = np.mean(np.square(y_test_norm - x_test_norm))\n",
    "\n",
    "# show test MSE\n",
    "print('-'*100)\n",
    "print(f'test mse of persistence = {test_mse_baseline}')\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test MSE of persistence is a number whose absolute value is not that important per se (because the input and output data have been normalised) but it will be useful to normalise the test MSE of our trained NNs.\n",
    "\n",
    "In the following cell, we compute the forecast skill of persistence. It will be illustrated in the following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute forecast skill\n",
    "fs_baseline = np.sqrt(np.mean(np.square(xt_fs-xt_fs[0]), axis=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> Example of surrogate model integration</span>\n",
    "\n",
    "In the following cell, we show one example of model integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the true and surrogate model integration for one trajectory\n",
    "plot_l96_compare_traj(\n",
    "    xt_fs[:, 0],\n",
    "    np.broadcast_to(xt_fs[0, 0], shape=xt_fs[:, 0].shape),\n",
    "    true_model,\n",
    "    linewidth=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Forecast skill</span>\n",
    "\n",
    "In the following cells, we plot the average forecast skill, normalised by the model variability. The shadow delimits the 90% confidence interval (percentiles 5 and 95)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the forecast skill\n",
    "plot_l96_forecast_skill(\n",
    "    dict(\n",
    "        persistence=fs_baseline,\n",
    "    ),\n",
    "    true_model,\n",
    "    p1=5,\n",
    "    p2=95,\n",
    "    xmax=4,\n",
    "    linewidth=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error rapidly grows as time evolves. After about $1$ Lyapunov time, the error oscillates around $\\sqrt{2}$, which is the theoretical asymptotic value due to the normalisation and which is consistent with the wave behaviour of the dynamics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <span style=\"color:green\"> A naive ML model </span>\n",
    "\n",
    "### <span style=\"color:blue\"> Construct and train the model</span>\n",
    "\n",
    "In this second test series, we train and evaluate a dense NN (sequential NN with only dense layers). In order to create this model, we use the [sequential API of tensorflow](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential).\n",
    "\n",
    "<span style=\"color:red\"> Exercise </span>\n",
    "- Implement the `make_sequential_network()` function, in which a \n",
    "  sequential neural network is created. The neural network should\n",
    "  take as input the current state and return the forecasted state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_sequential_network(seed, num_layers, num_nodes, activation):\n",
    "    \"\"\"Build a sequential neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int\n",
    "        The random seed.\n",
    "    num_layers : int\n",
    "        The number of hidden layers.\n",
    "    num_nodes : int\n",
    "        The number of nodes per hidden layer.\n",
    "    activation : str\n",
    "        The activation function for the hidden layers.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    network : tf.keras.Sequential\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    tf.keras.utils.set_random_seed(seed=seed)\n",
    "    # TODO: create a sequential network\n",
    "    network = tf.keras.models.Sequential()\n",
    "    # TODO: add the input layer\n",
    "    network.add(tf.keras.Input(shape=(true_model.Nx,)))\n",
    "    # TODO: add the hidden layers\n",
    "    for i in range(num_layers):\n",
    "        network.add(tf.keras.layers.Dense(num_nodes, activation=activation))\n",
    "    # TODO: add the output layer\n",
    "    network.add(tf.keras.layers.Dense(true_model.Nx))\n",
    "    # compile the neural network\n",
    "    network.compile(loss='mse', optimizer='adam')\n",
    "    # print short summary\n",
    "    network.summary()\n",
    "    # return the network\n",
    "    return network\n",
    "\n",
    "def train_network(seed, num_epochs, description, patience, model):\n",
    "    \"\"\"Train a neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int\n",
    "        The random seed.\n",
    "    num_epochs : int\n",
    "        The number of epochs.\n",
    "    description : str\n",
    "        The progress bar description.\n",
    "    patience : int\n",
    "        The patience for EarlyStopping.\n",
    "    model : tf.keras.Model\n",
    "        The network to train.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    history : dict\n",
    "        The training history.\n",
    "    \"\"\"\n",
    "    # train the ML model\n",
    "    # set random seed\n",
    "    tf.keras.utils.set_random_seed(seed=seed)\n",
    "    # tqdm callback\n",
    "    tqdm_callback = TQDMCallback(description)\n",
    "    # early stopping callback\n",
    "    early_stopping_callback = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=patience,\n",
    "        verbose=0,\n",
    "        restore_best_weights=True,\n",
    "    )\n",
    "    fit = model.fit(\n",
    "        x_train_norm, \n",
    "        y_train_norm,\n",
    "        verbose=0,\n",
    "        epochs=num_epochs, \n",
    "        validation_data=(x_valid_norm, y_valid_norm),\n",
    "        callbacks=[tqdm_callback, early_stopping_callback],\n",
    "    )\n",
    "    return fit.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we build a dense NN with $4$ internal layers and $128$ nodes per layer. The total number of parameters of this model is $59944$. This is actually quite large for a $40$-variable system. This is because the dense architecture is rather \"inefficient\" in terms of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the naive neural network\n",
    "naive_network = make_sequential_network(seed=314159, num_layers=4, num_nodes=128, activation='relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we train the model for $256$ epochs. We use an EarlyStopping callback to end the training when the validation loss stops improving. This should avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "fit_naive = train_network(\n",
    "    seed=3141592, \n",
    "    num_epochs=256, \n",
    "    description='naive NN training', \n",
    "    patience=16, \n",
    "    model=naive_network,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we plot the training history, that is, the evolution of the training MSE (the `loss`) and the validation MSE (the `val_loss`) as a function of the number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning history\n",
    "plot_learning_curve(\n",
    "    fit_naive['loss'],\n",
    "    fit_naive['val_loss'],\n",
    "    title='Naive NN training',\n",
    "    linewidth=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both values are visually closely related. The validation MSE is more noisy than the training MSE, which is expected because the training data is ten times as large as the validation data. After several epochs, the validation MSE gets a bit higher than the training MSE. This is explained by the fact that this data is not used in the gradient descent algorithm. Finally, at the end the validation MSE stops improving. This is the sign that the model is starting to overfit the training data and that we should stop the training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> Evaluate the model</span>\n",
    "\n",
    "We now compute the test MSE to evaluate our surrogate model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute test MSE\n",
    "test_mse_naive = naive_network.evaluate(x_test_norm, y_test_norm, verbose=0, batch_size=x_test_norm.shape[0])\n",
    "\n",
    "# show test MSE\n",
    "print('-'*100)\n",
    "print(f'test mse of persistence = {test_mse_baseline}')\n",
    "print(f'test mse of naive model = {test_mse_naive}')\n",
    "print()\n",
    "print(f'relative test mse of naive model = {test_mse_naive/test_mse_baseline}')\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain a reduction of about 80%, which is already quite good, but we will see later that it is possible to do much better.\n",
    "\n",
    "We now compute and illustrate the forecast skill of the neural network.\n",
    "\n",
    "<span style=\"color:red\"> Exercise </span>\n",
    "- Implement the `compute_forecast_skill()` function, in which we \n",
    "  use a surrogate model to predict the trajectories and then\n",
    "  compute the forecast skill. Use the `predict()` method of\n",
    "  `tf.keras.Model` inside a `for-loop`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_forecast_skill(model):\n",
    "    \"\"\"Compute the forecast skill.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : tf.keras.Model\n",
    "        The model to evaluate.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    fs : np.ndarray, shape (Nt+1, Ne)\n",
    "        The forecast skill.\n",
    "    \"\"\"\n",
    "    # allocate memory\n",
    "    (Nt, Ne, Nx) = xt_fs.shape\n",
    "    xt = np.zeros((Nt, Ne, Nx))\n",
    "    \n",
    "    # initialisation\n",
    "    xt[0] = xt_fs[0]\n",
    "    \n",
    "    # TODO: implement the neural network integration\n",
    "    for t in trange(Nt-1, desc='surrogate model integration'):\n",
    "        x_norm = normalise_x(xt[t])\n",
    "        y_norm = model.predict(x_norm, batch_size=Ne, verbose=0)\n",
    "        xt[t+1] = denormalise_y(y_norm)\n",
    "        \n",
    "    # compute and return the forecast skill\n",
    "    fs = np.sqrt(np.mean(np.square(xt_fs-xt), axis=2))\n",
    "    return (xt, fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute forecast skill\n",
    "xt_naive, fs_naive = compute_forecast_skill(naive_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> Example of surrogate model integration</span>\n",
    "\n",
    "In the following cell, we show once again one example of model integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the true and surrogate model integration for one trajectory\n",
    "plot_l96_compare_traj(\n",
    "    xt_fs[:, 0],\n",
    "    xt_naive[:, 0],\n",
    "    true_model,\n",
    "    linewidth=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error is lower than in the first test series, but only during the first few integration steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> Forecast skill</span>\n",
    "\n",
    "In the following cell, we plot once again the average forecast skill, normalised by the model variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the forecast skill\n",
    "plot_l96_forecast_skill(\n",
    "    dict(\n",
    "        persistence=fs_baseline,\n",
    "        naive=fs_naive,\n",
    "    ),\n",
    "    true_model,\n",
    "    p1=5,\n",
    "    p2=95,\n",
    "    xmax=4,\n",
    "    linewidth=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This curve confirms that the naive surrogate model is more accurate than persistence for one integration step, and that it remains more accurate until about $2$ Lyapunov times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:green\"> A smart ML model </span>\n",
    "\n",
    "### <span style=\"color:blue\"> Build and train the model</span>\n",
    "\n",
    "In this third and last test series, we train and evaluate a smart NN. This NN uses a sparse architecture with convolutional NN and controlled nonlinearity to reproduce the **model tendencies**, as well as a Runge-Kutta integration scheme to **emulate the dynamics**. In order to implement this NN, we use both the [functional API](https://www.tensorflow.org/guide/keras/functional) (for the model tendency) and the [subclassing API](https://www.tensorflow.org/guide/keras/custom_layers_and_models) (for the integration scheme) of tensorflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmartNetwork(tf.keras.Model):\n",
    "    \"\"\"Smart neural network for the Lorenz 1996 model.\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    dt : float\n",
    "        The integration time step.\n",
    "    tendency : tf.keras.Model\n",
    "        The network to compute the tendencies.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_filters, kernel_size, dt=0.05, **kwargs):\n",
    "        \"\"\"Initialise the smart network.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        num_filters : int\n",
    "            Number of filters to use in the convolutional layer.\n",
    "        kernel_size : int\n",
    "            Size of the convolutional kernel.\n",
    "        dt : float\n",
    "            Integration time step.\n",
    "        kwargs : dict\n",
    "            Additional parameters forwarded to `tf.keras.Model.__init__()`.\n",
    "        \"\"\"\n",
    "        super().__init__(**kwargs)\n",
    "        self.dt = dt\n",
    "        \n",
    "        # reshape layers\n",
    "        reshape_input = tf.keras.layers.Reshape((true_model.Nx, 1))\n",
    "        reshape_output = tf.keras.layers.Reshape((true_model.Nx,))\n",
    "        \n",
    "        # padding layer\n",
    "        border = kernel_size//2\n",
    "        def apply_padding(x):\n",
    "            x_left = x[..., -border:, :]\n",
    "            x_right = x[..., :border, :]\n",
    "            return tf.concat([x_left, x, x_right], axis=-2)\n",
    "        padding_layer = tf.keras.layers.Lambda(apply_padding)\n",
    "        \n",
    "        # convolutional layers\n",
    "        conv_layer_1 = tf.keras.layers.Conv1D(num_filters, kernel_size)\n",
    "        conv_layer_2 = tf.keras.layers.Conv1D(1, 1)\n",
    "        \n",
    "        # network for the model tendencies\n",
    "        x_in = tf.keras.Input(shape=(true_model.Nx,))\n",
    "        # reshape the input to be able to use convolutional layers\n",
    "        x = reshape_input(x_in)\n",
    "        # apply convolution with periodic padding\n",
    "        x = padding_layer(x)\n",
    "        x1 = conv_layer_1(x)\n",
    "        # construct non-linear terms\n",
    "        x2 = x1 * x1\n",
    "        # concatenate linear and non-linear terms\n",
    "        x3 = tf.concat([x1, x2], axis=-1)\n",
    "        # combine all channels into one\n",
    "        # there is no actual convolution here \n",
    "        # because the kernel_size is one for this layer\n",
    "        x_out = conv_layer_2(x3)\n",
    "        # reshape the output after the convolutional layers\n",
    "        x_out = reshape_output(x_out)\n",
    "        # pack everything into a tf.keras.Model\n",
    "        self.tendency = tf.keras.Model(inputs=x_in, outputs=x_out)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        \"\"\"Apply the network.\"\"\"\n",
    "        dx_dt_0 = self.tendency(x)\n",
    "        dx_dt_1 = self.tendency(x+0.5*self.dt*dx_dt_0)\n",
    "        dx_dt_2 = self.tendency(x+0.5*self.dt*dx_dt_1)\n",
    "        dx_dt_3 = self.tendency(x+self.dt*dx_dt_2)\n",
    "        dx_dt =  (dx_dt_0 + 2*dx_dt_1 + 2*dx_dt_2 + dx_dt_3)/6\n",
    "        return x + self.dt*dx_dt\n",
    "    \n",
    "def make_smart_network(seed, num_filters, kernel_size):\n",
    "    \"\"\"Build a sequential neural network.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    seed : int\n",
    "        The random seed.\n",
    "    num_filters : int\n",
    "        The number of filters.\n",
    "    kernel_size : int\n",
    "        The convolution kernel.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    network : SmartNetwork\n",
    "        The smart network.\n",
    "    \"\"\"\n",
    "    # set seed\n",
    "    tf.keras.utils.set_random_seed(seed=seed)\n",
    "    # create the network\n",
    "    network = SmartNetwork(\n",
    "        num_filters=num_filters, \n",
    "        kernel_size=kernel_size, \n",
    "        dt=true_model.dt,\n",
    "    )\n",
    "    # compile the neural network\n",
    "    network.compile(loss='mse', optimizer='adam')\n",
    "    # print short summary\n",
    "    network.tendency.summary()\n",
    "    # return the network\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct the smart neural network\n",
    "smart_network = make_smart_network(seed=31415926, num_filters=6, kernel_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total number of parameters is only $49$. Furthermore in this case, with well-chosen parameters it is possible to reproduce the true dynamics up to machine precision: the model is said to be **identifiable**. Also note that this network is built in such a way that we don't need the input and output data to be normalised.\n",
    "\n",
    "In the following cell, we train the model for up to $128$ epochs. Once again, we use an EarlyStopping callback to end the training when the validation loss stops improving in order to avoid overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the network\n",
    "fit_smart = train_network(\n",
    "    seed=314159265, \n",
    "    num_epochs=128, \n",
    "    description='smart NN training', \n",
    "    patience=8, \n",
    "    model=smart_network,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell we plot the training history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning history\n",
    "plot_learning_curve(\n",
    "    fit_smart['loss'],\n",
    "    fit_smart['val_loss'],\n",
    "    title='Smart NN training',\n",
    "    linewidth=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the training and validation MSE are visually closely related. However, by contrast with the previous test series, after about $35$ epochs, the MSEs have decreased to $10^{-9}$, which should be very close to the numerical precision zero (tensorflow is working on simple precision for real numbers). Passed $40$ epochs, the MSEs oscillate at very low values. This behaviour can be considered as numerical noise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue\"> Evaluate the model</span>\n",
    "\n",
    "We now compute the test MSE to evaluate our surrogate model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute test MSE\n",
    "test_mse_smart = smart_network.evaluate(x_test_norm, y_test_norm, verbose=0, batch_size=x_test_norm.shape[0])\n",
    "\n",
    "# show test MSE\n",
    "print('-'*100)\n",
    "print(f'test mse of persistence = {test_mse_baseline}')\n",
    "print(f'test mse of naive model = {test_mse_naive}')\n",
    "print(f'test mse of smart model = {test_mse_smart}')\n",
    "print()\n",
    "print(f'relative test mse of naive model = {test_mse_naive/test_mse_baseline}')\n",
    "print(f'relative test mse of smart model = {test_mse_smart/test_mse_baseline}')\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test MSE is sufficiently close to zero so that we can consider that our surrogate model reproduces the true model dynamics up to numerical precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute forecast skill\n",
    "xt_smart, fs_smart = compute_forecast_skill(smart_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> Example of surrogate model integration</span>\n",
    "\n",
    "In the following cell, we show once again one example of model integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the true and surrogate model integration for one trajectory\n",
    "plot_l96_compare_traj(\n",
    "    xt_fs[:, 0],\n",
    "    xt_smart[:, 0],\n",
    "    true_model,\n",
    "    linewidth=18,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, the error is so low that it is not visible until about $4$ MTU. At that time, the true model trajectory and the surrogate model trajectory diverge from each other. Indeed, the two models are equivalent up to numerical precision, but they are not bit-wise equivalent, which means that this divergence is unavoidable because of the chaotic nature of the dynamics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### <span style=\"color:blue\"> Forecast skill</span>\n",
    "\n",
    "In the following cell, we plot once again the average forecast skill, normalised by the model variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the forecast skill\n",
    "plot_l96_forecast_skill(\n",
    "    dict(\n",
    "        persistence=fs_baseline,\n",
    "        naive=fs_naive,\n",
    "        smart=fs_smart,\n",
    "    ),\n",
    "    true_model,\n",
    "    p1=5,\n",
    "    p2=95,\n",
    "    xmax=30,\n",
    "    linewidth=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This curve confirms that the the smart surrogate model is equivalent to the true model up to numerical precision. The numerical divergence between the true and surrogate model happens on average after about $5$ Lyapunov times."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
